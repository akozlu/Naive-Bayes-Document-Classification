Until recently I always understood the term "kingdom theology" to mean the 
theology of the kingdom of God, but now I have discovered that there is a 
new and more specialized meaning. I gather that it is also called "Dominion 
theology", and that it has to do with a belief that Christians must create a 
theocratic form of government on earth before Christ will come again.

I have not come across anyone who believes or advocates this, but I am told 
that it is a very widespread belief in the USA.

Can anyone give me any more information about it?

Here are some of my questions:

1. Is it the teaching of any particular denomination? If so, which?
2. Where and when does it start?
3. Are there any particular publications that propagate it?
4. Are there any organizations that propagate it?